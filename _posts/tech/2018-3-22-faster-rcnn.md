---
title:  Faster R-CNN 检测微小物体
layout: blog
background-image: http://static.zybuluo.com/sixijinling/9khganmk3o5gosnvde9l78o2/image_1c8u6de3r12s5oeog9i1ij6d1g9.png
tech: true
mathjax: true
date: 2018-3-22 18:26
category: 技术
description:  Faster R-CNN用来识别大件物体效果好，但是小物体效果较差，因此想了一些改进的方法。
published: true
comment: true
github_comments_issueid: "1"
tags:
- object detection
---

本文基于[Faster R-CNN的pytorch实现][1]，[原始论文][2]，需要安装：

```
pip install easydict
```

## 数据输入：使用自己的数据设计yourdata.py

参考dataset目录下的其他数据集的类进行修改：

- `_classes`：所有框的类别，比较特殊的就是`__background__`

```
# Pascal_VOC
self._classes = ('__background__',  # always index 0
                 'aeroplane', 'bicycle', 'bird', 'boat',
                 'bottle', 'bus', 'car', 'cat', 'chair',
                 'cow', 'diningtable', 'dog', 'horse',
                 'motorbike', 'person', 'pottedplant',
                 'sheep', 'sofa', 'train', 'tvmonitor')
```
- 关键要修改的函数就是`_load_XXX_annotation(self, index)`（XXX是你的数据集的名字），要实现的功能就是给定image的index，返回所有的bounding box标注。
    - 返回的东西长这样：
```
    return {'width': width,
            'height': height,
            'boxes': np.array(boxes),
            'gt_classes': np.array(gt_classes),
            'gt_overlaps': overlaps,
            'flipped': False,
            'seg_areas': np.array(seg_areas)}
```
- `flipped`：默认是horizontal flip，我自己加了vertical flip。
    - 源代码将image_id * 2作为翻转后的图像的index，因此还是要把数据集的index单独存数字id，通过id索引path
    - 同时修改`dataset/minibatch.py`来支持上下翻转
- `gt_overlaps`:Crowd instances are
    handled by marking their overlaps (with all categories) to -1. This overlap value means that crowd "instances" are excluded from training.
- `seg_areas`:

### 图像预处理

- 减去的均值是固定的$3 \times 1$向量；
- 图像rescale到显存支持的一定大小。 `targetSize` 和 `maxSize` 默认是 600 和 1000 ，根据显存大小和图片大小来设计就好（比如最长边限制在2100或者短边是1700的时候一块gpu只能跑一个sample）。
    - 代码里用`need_crop`来标识是否需要rescale

![此处输入图片的描述][3]

#网络结构

R-CNN包括三种主要网络：

1. Head：输入(w,h,3)生成feature map，降采样了16倍；
    - w和h是预处理以后的图片大小哦
2. Region Proposal Network (RPN)：基于feature map，预测ROI；
    - `Crop Pooling`：从feature map中crop相应位置
3. Classification Network：对crop出的区域进行分类。

![此处输入图片的描述][4]

先来看看Head怎么得到feature map。拿VGG16作为backbone来举例的话，一个完整的VGG16网络长这样：

![VGG16][5]

其中红色部分就是下采样的时刻。原始论文里使用VGG16，因为提feature map只用了最后一次max pooling前面的部分，所以留下来的四次pooling总共下采样是16倍。得到的feature map长这样：

![under sampling][6]

最终的feature映射回原图的话大概长这样：

![此处输入图片的描述][7]

有了feature map以后，开始走RCNN的主体流程：

![此处输入图片的描述][8]

### 1. Anchor Generation Layer

第一步就是生成anchor，这里的anchor亦可理解为bounding box。rpn的任务是对上上图的每个小红点都计算若干anchor（默认是9个）：

![此处输入图片的描述][9]

-  三种颜色分别代表128x128, 256x256, 512x512
-  每种颜色的三个框分别代表比例1:1, 1:2 and 2:1

> anchor是RPN的windows的大小，在feature map的每一个位置使用不同尺度和长宽比的window提取特征。原论文描述为“a pyramid of regression references”。

对应到代码上：在trainval_net.py中，imagenet的`ANCHOR_SCALES`的默认是[4, 8, 16, 32]，`ANCHOR_RATIOS`默认是[0.5,1,2]。但是并不是越多越好，要控制在一定的数量，理由是：

1. anchors多了就会造成faster RCNN的时间复杂度提高，anchors多的最极端情况就是overfeats中sliding windows。
2. 使用多尺度的anchors未必全部对scale-invariant property都有贡献

所以，在使用自己的数据的时候，统计一下gorund truth框的大小（注意考虑预处理rescale的系数），确定大小范围还是很有必要的。

### 2. Region Proposal Layer

先明确foreground和background的概念：前景`fg`（foreground）代表有物体（不管是哪个类别），背景`bg`（background）就是没有任何物体。

前一步生成anchor得到的是dense candidate region，rpn根据region是fg的概率来对所有region排序。

Region Proposal Layer的两个任务就是：

- `rpn_cls_score`:判断anchor是前景还是背景
- `rpn_bbox_pred`:根据**regression coefficient**调整anchor的位置、长宽，从而改进anchor，比如让它们更贴合物体边界。

![此处输入图片的描述][10]

值得注意的是，这里的anchor是以降采样16倍得到的feature map为基础的，所以总共是$\frac w {16} * \frac h {16} * 9$个anchor。每个anchor唯一对应着一个class score和bounding box regressor。

#### Proposal Layer

基于fg的score，使用nms来筛除多余的anchor

![此处输入图片的描述][11]

#### Anchor Target Layer

计算RPN loss：

![此处输入图片的描述][12]

- Classification Loss: cross_entropy(predicted _class, actual_class)
- Bounding Box Regression Loss:![此处输入图片的描述][13]
![此处输入图片的描述][14]
![此处输入图片的描述][15]

![此处输入图片的描述][16]

需要注意的是，fg/bg并不是“非黑即白”，而是有“don't care”这单独的一类，用来标识既不是fg也不是bg的box，这些框也就不在loss的计算范围中。同时，“don't care”也用来约束fg和bg的总数和比例，比如多余的fg随机标为“don't care”。

一些相关参数：

- `TRAIN.RPN_POSITIVE_OVERLAP`: 用来筛选fg box的阈值(Default: 0.7)
- `TRAIN.RPN_NEGATIVE_OVERLAP`: 用来筛选bg box的阈值(Default: 0.3)。这样一来，和ground truth的overlap在0.3~0.7就是“don't care”。
- `TRAIN.RPN_BATCHSIZE`: fg和bg anchor的总数 (default: 256)
- `TRAIN.RPN_FG_FRACTION`: batch size中fg的比例 (default: 0.5)。如果fg数量超过 TRAIN.RPN_BATCHSIZE$\times$ TRAIN.RPN_FG_FRACTION, 超过的部分 (根据索引随机选择) 就被标为 “don’t care”.

输入:

- RPN Network Outputs (predicted foreground/background class labels, regression coefficients)
- Anchor boxes (generated by the anchor generation layer)
- Ground truth boxes

输出：

- Good foreground/background boxes and associated class labels
- Target regression coefficients

#### Proposal Target Layer

proposal layer产生ROI list，而Proposal Target Layer负责从这个list中选出可信的ROI。这些ROI经过 crop pooling从feature map中crop出相应区域，传给后面的classification
layer（head_to_tail）.

计算所有ROI和所有ground truth的max overlap，从而判断是fg还是bg。这里用到了两个参数：

- `TRAIN.FG_THRESH`：用来判断fg ROI（default：0.5）
- `TRAIN.BG_THRESH_LO` ~ `TRAIN.BG_THRESH_HI`：这个区间的是bg ROI。(default 0.1, 0.5 respectively)

这样的设计可以看作是 “hard negative mining” ，用来给classifier投喂更难的bg样本。

> 为了保证fg和bg的总数是恒定的（N），如果bg太少了，就会随机重复一些bg样本来弥补差额。

![此处输入图片的描述][17]

神奇的一点：在后面的classification layer中，这个bbox_inside_weights起一个mask的作用，这样就只计算fg的，不管bg的，但是算分类loss的时候还是都考虑。

输入:

- ROIs produced by the proposal layer
- ground truth information

输出:

- Selected foreground and background ROIs that meet overlap criteria.
- Class specific target regression coefficients for the ROIs

Parameters:

- `TRAIN.BATCH_SIZE`: (default 128) 所选fg和bg box的最大数量.
- `TRAIN.FG_FRACTION`: (default 0.25). fg box 不能超过 BATCH_SIZE*FG_FRACTION

### ROI Pooling Layer

简言之就是负责从feature map中提取ROI对应区域。

![此处输入图片的描述][18]

Pytorch的**torch.nn.functional.affine_grid** 和torch.nn.functional.grid_sample

crop pooling的步骤：

1. ROI坐标 $\div$ head网络下采样的倍数（也就是stride）。需要特别指出的是：proposal target layer给出的ROI坐标是原图尺度上的（默认800$\times$600），因此映射到feature map之前要先除stride（默认是16，前面解释过）。
2.  affine transformation matrix（仿射变换矩阵）
3.  最关键的一点在于后面的分类网接收的是固定大小输入，因此这一步需要把矩形窗

![此处输入图片的描述][19]

### Classification Layer

![此处输入图片的描述][20]

fc之后得到的一维特征向量被送到两个全连接网络中：
![此处输入图片的描述][21]

- `cls_score_net`：生成roi每个类别的score（softmax之后就是概率了）
- `bbox_pred_net`：结合两者得到最终的bbox坐标
    - class specific bounding box regression coefficients
    - 原来proposal target layer生成的 bbox 坐标

这个阶段要把不同物体的标签考虑进来了，所以是一个多分类问题。使用交叉熵计算分类Loss：

![此处输入图片的描述][22]

这个阶段依然要计算bounding box regression loss，和前面的R区别PN的区别在于：

- RPN的是为了让bbox更紧凑地贴合物体。 anchor target layer算得的target regression coefficients需要将roi box和离它最近的ground truth bbox对齐。
- classification layer中的是针对各个类别的。也就是说，对每个roi、每个类别都会生成一套coefficient。只有那些分类正确了的box才能参与，分类错误的直接不考虑了。

> 代码实现上：使用了mask array将for循环操作转化成了矩阵乘法。mask array标注了每个anchor的正确物体类别。

有意思的是，训练classification layer得到的loss也会反向传播给RPN。这是因为用来做crop pooling的ROI box坐标不仅是RPN产生的 regression coefficients应用到anchor box上的结果，其本身也是网络输出。因此，在反传的时候，误差会通过roi pooling layer传播到RPN layer。好在crop pooling在Pytorch中有内部实现，省去了计算梯度的麻烦。

## Inference

![此处输入图片的描述][23]

## 针对小物体

每个小红点之间就是16像素的间隔了，如果要检测特别细小的物体，这么大的下采样就很危险了。于是，为了尽量不破坏细小物体的清晰度，参考[github上关于检测微小物体的讨论][24]，我尝试了两种方案：

### 1. 降低下采样

* 一旦改变下采样，同时还要改变feature_stride和spatial_scale。否则预测框框就变成了边缘的线条（同时loss_rpn_cls奇高）。比如VGG16（降采样8倍）：
    * __C.feat_stride': 8”
    * self.spatial_scale to (1/8)
*  vgg16尝试4倍下采样：去掉了stage5的卷积，保障输入图像最短边在1200

#### 剪裁网络

[把卷积层变成全连接层][25]

```
list(model.modules()) # to inspect the modules of your model
my_model = nn.Sequential(*list(model.modules())[:-1]) # strips off last linear layer
```

### 2. 切割原图

- 训练数据切patch：将原图切分为四等份再训练，保障大部分图的清晰度不被压缩。这样一来，标注也要做调整：
    - 一种方式是重新生成新的子图标注，新写一个yourdata.py；
    - 另一种偷懒方式则是修改 yourdata.py的`_load_XXX_anotation(self, index)`，使得读入每个子图，返回的也是每个子图的所有标注框。
- 测试数据为原图，沿用原来的数据读入方式即可。

# Appendix

## non-maximum suppression（nms）

![此处输入图片的描述][26]

上图左边的黑色数字代表fg的概率

- standard NMS (boxes are ranked by y coordinate of bottom right corner). This results in the box with a lower score being retained. The second figure uses modified NMS (boxes are ranked by foreground scores).

![此处输入图片的描述][27]

- This results in the box with the highest foreground score being retained, which is more desirable. In both cases, the overlap between the boxes is assumed to be higher than the NMS overlap threhold.

[讲nms（non-maximum suppression）的文章][28]

## Resnet50

![此处输入图片的描述][29]

![此处输入图片的描述][30]
* resnet的downsample：第一个卷积层就降采样了2倍
* Resnet18，降低下采样，增加卷积层
![image_1c8u6de3r12s5oeog9i1ij6d1g9.png-183.1kB][31]
检查网络输出

## FPN
![image_1c8u6egich3e1ha41d3o1m22s3g16.png-228.8kB][32]
![image_1c8u6f7esu9i1sl510hs1pnm1qvl1j.png-30.8kB][33]
[原始论文][34]
[FPN的Pytorch实现][35]

### Focal loss

看ICCV那篇focal loss的论文

[Pytorch实现参考][36]


## Todo List

- [ ] face book开源的基于caffe2的detectron，也可以学习一下，里面有各种检测的算法，有mask rcnn神马的
- [关于mask rcnn实现的讨论][37]，kaggle上也有一个做医疗图像的demo
- [ ] 可能是resnet的bn_train设置为False，回头改一下
- 加入valid过程，确认是否有过拟合:爆显存了
- [ ] roibatchloader的问题：trim
- 遇到的bug
```
random.choice()
```
- negative sampling：selective search



## Reference

- 如果想了解object detection的发展史，可以看[Object Detection][38]
- 推荐阅读[Faster R-CNN: Down the rabbit hole of modern object detection][39]


  [1]: https://github.com/jwyang/faster-rcnn.pytorch
  [2]: https://arxiv.org/pdf/1506.01497.pdf
  [3]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa46e9e0bbd7.png
  [4]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5a9ffec911c19.png
  [5]: https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/vgg.b6e48b99.png
  [6]: https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/image-to-feature-map.89f5aecb.png
  [7]: https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-centers.141181d6.png
  [8]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa0053323ac5.png
  [9]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa05d3ecef3e.png
  [10]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa0695484e3e.png
  [11]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa5766d53b63.png
  [12]: http://www.telesens.co/wordpress/wp-content/ql-cache/quicklatex.com-142d5b70256748a64605bfc6e2f30ea9_l3.svg
  [13]: http://www.telesens.co/wordpress/wp-content/ql-cache/quicklatex.com-79e8cbe4b5682f6abc719c54d768a4ae_l3.svg
  [14]: http://www.telesens.co/wordpress/wp-content/ql-cache/quicklatex.com-f26b9d082be79d08e06cdbeb5cfc1e3a_l3.svg
  [15]: http://www.telesens.co/wordpress/wp-content/ql-cache/quicklatex.com-dae64c7ea8affa572e4b38b84688e1fd_l3.svg
  [16]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa13d4d911d3.png
  [17]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa32302afc0b.png
  [18]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa402baba3a1.png
  [19]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa4255fdacb6.png
  [20]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa55c81eac0a.png
  [21]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa55c97f3287.png
  [22]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa1cd250f265.png
  [23]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa70ff399c57.png
  [24]: https://github.com/rbgirshick/py-faster-rcnn/issues/86
  [25]: https://stackoverflow.com/questions/44146655/how-to-convert-pretrained-fc-layers-to-conv-layers-in-pytorch
  [26]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa7c84451f81.png
  [27]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa7c828703ab.png
  [28]: https://zhuanlan.zhihu.com/p/31427728
  [29]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa59c8da4c4b.png
  [30]: http://www.telesens.co/wordpress/wp-content/uploads/2018/03/img_5aa59d170c750.png
  [31]: http://static.zybuluo.com/sixijinling/9khganmk3o5gosnvde9l78o2/image_1c8u6de3r12s5oeog9i1ij6d1g9.png
  [32]: http://static.zybuluo.com/sixijinling/cv2l758k7dyj022k0iinwsm1/image_1c8u6egich3e1ha41d3o1m22s3g16.png
  [33]: http://static.zybuluo.com/sixijinling/elabj076z56xa1xsfbwqyy50/image_1c8u6f7esu9i1sl510hs1pnm1qvl1j.png
  [34]: https://arxiv.org/pdf/1612.03144.pdf
  [35]: https://github.com/jwyang/fpn.pytorch
  [36]: https://github.com/marvis/pytorch-yolo2/blob/master/FocalLoss.py
  [37]: http://forums.fast.ai/t/implementing-mask-r-cnn/2234
  [38]: https://tryolabs.com/blog/2017/08/30/object-detection-an-overview-in-the-age-of-deep-learning/
  [39]: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/